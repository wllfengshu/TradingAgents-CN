"""
GitHub Copilot é€‚é…å™¨
æ”¯æŒé€šè¿‡ GitHub Token è®¿é—®å¤šç§ä¼˜è´¨å¤§æ¨¡å‹

ğŸ”¹ æ–¹æ¡ˆ1: Azure AI Inference ç«¯ç‚¹ï¼ˆæ ‡å‡†æ¨¡å‹ï¼Œå¤‡ç”¨æ–¹æ¡ˆï¼‰:
- gpt-4o: GPT-4 Optimized â­ (æ¨èï¼Œé»˜è®¤)
- gpt-4.1: GPT-4.1
- gpt-4o-mini: GPT-4o Mini (å¿«é€Ÿä¸”ç»æµ)
- gpt-4: GPT-4
- gpt-3.5-turbo: GPT-3.5 Turbo

ğŸ”¸ æ–¹æ¡ˆ2: GitHub Copilot Chat API ç«¯ç‚¹ï¼ˆæ¨¡æ‹ŸIDEæ’ä»¶ï¼Œæ”¯æŒå…¨æ¨¡å‹ï¼‰:
- gpt-5: OpenAI GPT-5 â­ (æœ€æ–°æ——èˆ°æ¨¡å‹)
- gpt-4o: GPT-4 Optimized
- claude-sonnet-4.5: Claude Sonnet 4.5 â­ (Anthropicæœ€æ–°æ¨¡å‹)
- claude-3.5-sonnet: Claude 3.5 Sonnet
- o1-preview: OpenAI O1 Preview (æ¨ç†æ¨¡å‹)
- o1-mini: OpenAI O1 Mini (å¿«é€Ÿæ¨ç†)

ğŸ¯ æ™ºèƒ½ç«¯ç‚¹é€‰æ‹©ç­–ç•¥ï¼š
1. Claude æ¨¡å‹ â†’ å°è¯•ä½¿ç”¨ Copilot Chat API (æ–¹æ¡ˆ2ï¼Œâš ï¸ éœ€è¦ç‰¹æ®Štoken)
2. GPT-5 ç³»åˆ— â†’ å°è¯•ä½¿ç”¨ Copilot Chat API (æ–¹æ¡ˆ2ï¼Œâš ï¸ éœ€è¦ç‰¹æ®Štoken)
3. O1 ç³»åˆ— â†’ å°è¯•ä½¿ç”¨ Copilot Chat API (æ–¹æ¡ˆ2ï¼Œâš ï¸ éœ€è¦ç‰¹æ®Štoken)
4. å…¶ä»–æ¨¡å‹ â†’ ä½¿ç”¨ Azure AI Inference (æ–¹æ¡ˆ1ï¼Œâœ… æ¨è)

âš ï¸ é‡è¦æç¤ºï¼š
- æ–¹æ¡ˆ2éœ€è¦ä»IDEæ’ä»¶æå–Copilot session tokenï¼Œæ™®é€šGitHub tokenä¼šå¾—åˆ°403é”™è¯¯
- æ¨èä½¿ç”¨æ–¹æ¡ˆ1 (gpt-4o, gpt-5ç­‰æ¨¡å‹)ï¼Œé…ç½®ç®€å•ä¸”ç¨³å®š
- è¯¦ç»†è¯´æ˜è¯·å‚è€ƒ: docs/COPILOT_CLAUDE_SUPPORT.md

è·å– GitHub Token (æ–¹æ¡ˆ1):
1. å®‰è£… GitHub CLI: https://cli.github.com/
2. è¿è¡Œè®¤è¯: gh auth login
3. è·å– token: gh auth token
4. è®¾ç½®ç¯å¢ƒå˜é‡: GITHUB_COPILOT_TOKEN=your_token

æ¨èä½¿ç”¨æ–¹æ³•ï¼ˆæ–¹æ¡ˆ1ï¼‰:
```python
from tradingagents.llm_adapters.copilot_adapter import ChatCopilot

# æ¨èï¼šä½¿ç”¨ GPT-4o (æ–¹æ¡ˆ1)
llm = ChatCopilot(model="gpt-4o", temperature=0.7)
response = llm.invoke("ä½ å¥½")
```

é…ç½®æ–‡ä»¶ç¤ºä¾‹:
1. åœ¨ .env æ–‡ä»¶ä¸­é…ç½®:
   GITHUB_COPILOT_TOKEN=ghp_your_github_token_here
   GITHUB_COPILOT_MODEL=gpt-4o
   GITHUB_COPILOT_ENABLED=true

2. åœ¨ config/models.json ä¸­æ·»åŠ é…ç½®:
   {
     "provider": "copilot",
     "model_name": "gpt-4o",
     "api_key": "",
     "base_url": null,
     "max_tokens": 8000,
     "temperature": 0.7,
     "enabled": true
   }
"""

import os
import uuid
from typing import Optional, Dict, Any
from tradingagents.llm_adapters.openai_compatible_base import OpenAICompatibleBase

# å¯¼å…¥æ—¥å¿—æ¨¡å—
from tradingagents.utils.logging_manager import get_logger
logger = get_logger('agents')


class ChatCopilot(OpenAICompatibleBase):
    """
    GitHub Copilot å¤§æ¨¡å‹é€‚é…å™¨

    é€šè¿‡ GitHub Copilot API è®¿é—®å¤šç§ä¼˜è´¨æ¨¡å‹ï¼ŒåŒ…æ‹¬:
    - GPT-4o, GPT-5 (æ¨è)
    - Claude Sonnet 4.5, Claude 3.5 Sonnet â­
    - O1 ç³»åˆ—ç­‰

    åŒæ–¹æ¡ˆç­–ç•¥ï¼š
    - æ–¹æ¡ˆ1ï¼ˆå¤‡ç”¨ï¼‰: Azure AI Inference - é€‚ç”¨äºæ ‡å‡†GPTæ¨¡å‹
    - æ–¹æ¡ˆ2ï¼ˆä¸»è¦ï¼‰: Copilot Chat API - æ¨¡æ‹ŸIDEæ’ä»¶ï¼Œæ”¯æŒClaudeç­‰å…¨æ¨¡å‹
    """

    def __init__(
        self,
        model: str = "gpt-4o",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 1.0,
        max_tokens: Optional[int] = 8000,
        timeout: Optional[int] = 120,
        **kwargs
    ):
        """
        åˆå§‹åŒ– GitHub Copilot é€‚é…å™¨

        Args:
            model: æ¨¡å‹åç§° (æ”¯æŒ: gpt-4o, gpt-5, claude-sonnet-4.5, claude-3.5-sonnet, o1-previewç­‰)
            api_key: GitHub Token (å¦‚æœä¸æä¾›åˆ™ä»ç¯å¢ƒå˜é‡ GITHUB_COPILOT_TOKEN è·å–)
            base_url: API åŸºç¡€ URL (None=è‡ªåŠ¨é€‰æ‹©)
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            timeout: è¯·æ±‚è¶…æ—¶æ—¶é—´(ç§’)
            **kwargs: å…¶ä»–å‚æ•°
        """

        # ğŸ¯ æ™ºèƒ½ç«¯ç‚¹é€‰æ‹©ç­–ç•¥ï¼ˆåŒæ–¹æ¡ˆï¼‰
        # æ–¹æ¡ˆ1: Azure AI Inference (å¤‡ç”¨ï¼Œå‘åå…¼å®¹)
        # æ–¹æ¡ˆ2: Copilot Chat API (ä¸»è¦ï¼Œæ¨¡æ‹ŸIDEæ’ä»¶)
        use_chat_api = False  # æ ‡è®°æ˜¯å¦ä½¿ç”¨Chat API

        if not base_url:
            # éœ€è¦ä½¿ç”¨ Copilot Chat API çš„æ¨¡å‹ï¼ˆæ–¹æ¡ˆ2ï¼‰
            chat_api_models = [
                "claude",           # æ‰€æœ‰Claudeæ¨¡å‹
                "gpt-5",           # GPT-5ç³»åˆ—
                "o1-preview",      # O1ç³»åˆ—
                "o1-mini"
            ]

            # æ£€æŸ¥æ¨¡å‹æ˜¯å¦éœ€è¦ä½¿ç”¨ Chat API
            if any(m in model.lower() for m in chat_api_models):
                # ä½¿ç”¨æ–¹æ¡ˆ2: GitHub Copilot Chat API (æ¨¡æ‹ŸIDEæ’ä»¶)
                # æ³¨æ„ï¼šåªä½¿ç”¨åŸºç¡€URLï¼ŒOpenAI SDKä¼šè‡ªåŠ¨æ·»åŠ  /chat/completions
                base_url = "https://api.githubcopilot.com"
                use_chat_api = True
                logger.info(f"ğŸ¯ [Copiloté€‚é…å™¨] æ£€æµ‹åˆ° {model}ï¼Œä½¿ç”¨æ–¹æ¡ˆ2: Copilot Chat API")
                logger.info(f"ğŸ”§ [Copiloté€‚é…å™¨] æ¨¡æ‹ŸIDEæ’ä»¶æ¨¡å¼ï¼Œæ”¯æŒClaudeç­‰å…¨æ¨¡å‹")
            else:
                # ä½¿ç”¨æ–¹æ¡ˆ1: Azure AI Inference (å¤‡ç”¨ï¼Œå‘åå…¼å®¹)
                base_url = "https://models.inference.ai.azure.com"
                logger.info(f"ğŸ”§ [Copiloté€‚é…å™¨] ä½¿ç”¨æ–¹æ¡ˆ1: Azure AI Inference (å¤‡ç”¨)")

        logger.info(f"ğŸš€ [Copiloté€‚é…å™¨] åˆå§‹åŒ– GitHub Copilot é€‚é…å™¨")
        logger.info(f"ğŸ“¦ [Copiloté€‚é…å™¨] æ¨¡å‹: {model}")
        logger.info(f"ğŸŒ [Copiloté€‚é…å™¨] APIåœ°å€: {base_url}")

        # Azure AI Inference çš„é™åˆ¶ï¼šæŸäº›æ¨¡å‹åªæ”¯æŒ temperature=1
        # ä»…åœ¨æ–¹æ¡ˆ1ä¸‹åº”ç”¨æ­¤é™åˆ¶
        if "azure.com" in base_url.lower():
            if temperature != 1.0:
                logger.warning(f"âš ï¸  [Copiloté€‚é…å™¨] æ–¹æ¡ˆ1é™åˆ¶: temperature ä» {temperature} é‡ç½®ä¸º 1.0")
                temperature = 1.0

        # GPT-5 ç³»åˆ—å’Œ O1 ç³»åˆ—çš„ç‰¹æ®Šå¤„ç†
        # è¿™äº›æ¨¡å‹ä½¿ç”¨ max_completion_tokens è€Œä¸æ˜¯ max_tokens
        init_kwargs = kwargs.copy()
        if "gpt-5" in model.lower() or "o1" in model.lower():
            logger.info(f"ğŸ”§ [Copiloté€‚é…å™¨] {model} ä½¿ç”¨ max_completion_tokens å‚æ•°")
            max_tokens = None
            if 'model_kwargs' not in init_kwargs:
                init_kwargs['model_kwargs'] = {}
            init_kwargs['model_kwargs']['max_completion_tokens'] = 8000

        # ğŸ¨ å¦‚æœä½¿ç”¨ Chat API (æ–¹æ¡ˆ2)ï¼Œæ·»åŠ IDEæ’ä»¶æ¨¡æ‹Ÿheaders
        if use_chat_api or "api.githubcopilot.com" in base_url:
            logger.info(f"ğŸ­ [Copiloté€‚é…å™¨] å¯ç”¨IDEæ’ä»¶æ¨¡æ‹Ÿæ¨¡å¼")

            # æ¨¡æ‹Ÿ JetBrains IDE çš„ Copilot æ’ä»¶
            # é€šè¿‡ model_kwargs ä¼ é€’é¢å¤–çš„è¯·æ±‚å‚æ•°
            if 'model_kwargs' not in init_kwargs:
                init_kwargs['model_kwargs'] = {}

            # ç”Ÿæˆå”¯ä¸€çš„ä¼šè¯ID
            session_id = str(uuid.uuid4())
            machine_id = str(uuid.uuid4())

            # æ³¨æ„ï¼šOpenAI SDK ä¼šè‡ªåŠ¨å¤„ç†æŸäº›headers
            # æˆ‘ä»¬é€šè¿‡ default_headers å‚æ•°ä¼ é€’è‡ªå®šä¹‰headers
            ide_headers = {
                "Editor-Version": "PyCharm/2024.2",
                "Editor-Plugin-Version": "copilot-intellij/1.5.0",
                "OpenAI-Intent": "conversation-panel",
                "VScode-SessionId": session_id,
                "VScode-MachineId": machine_id,
                "User-Agent": "GithubCopilot/1.5.0",
            }

            # LangChainçš„ChatOpenAIæ”¯æŒdefault_headerså‚æ•°
            init_kwargs['default_headers'] = ide_headers
            logger.info(f"âœ… [Copiloté€‚é…å™¨] IDE headerså·²é…ç½®: {list(ide_headers.keys())}")

        # è°ƒç”¨çˆ¶ç±»åˆå§‹åŒ–
        try:
            super().__init__(
                provider_name="copilot",
                model=model,
                api_key_env_var="GITHUB_COPILOT_TOKEN",
                base_url=base_url,
                api_key=api_key,
                temperature=temperature,
                max_tokens=max_tokens,
                timeout=timeout,
                **init_kwargs
            )
            logger.info(f"âœ… [Copiloté€‚é…å™¨] GitHub Copilot é€‚é…å™¨åˆå§‹åŒ–æˆåŠŸ")

            # å¦‚æœä½¿ç”¨ Chat APIï¼Œç»™å‡ºé‡è¦æç¤º
            if use_chat_api:
                logger.warning(f"âš ï¸  [Copiloté€‚é…å™¨] æ³¨æ„: {model} ä½¿ç”¨æ–¹æ¡ˆ2 (Copilot Chat API)")
                logger.warning(f"âš ï¸  [Copiloté€‚é…å™¨] æ­¤æ–¹æ¡ˆéœ€è¦ Copilot session tokenï¼Œä¸èƒ½ä½¿ç”¨æ™®é€š GitHub token")
                logger.warning(f"âš ï¸  [Copiloté€‚é…å™¨] å¦‚é‡åˆ° 403 é”™è¯¯ï¼Œå»ºè®®æ”¹ç”¨ gpt-4o ç­‰æ–¹æ¡ˆ1æ”¯æŒçš„æ¨¡å‹")
                logger.warning(f"âš ï¸  [Copiloté€‚é…å™¨] è¯¦è§æ–‡æ¡£: docs/COPILOT_CLAUDE_SUPPORT.md")

        except Exception as e:
            if use_chat_api and "403" in str(e):
                logger.error(f"âŒ [Copiloté€‚é…å™¨] æ–¹æ¡ˆ2è®¤è¯å¤±è´¥ (403 Forbidden)")
                logger.error(f"ğŸ’¡ [Copiloté€‚é…å™¨] è§£å†³æ–¹æ¡ˆ:")
                logger.error(f"   1. æ¨è: æ”¹ç”¨ gpt-4o æ¨¡å‹ (æ–¹æ¡ˆ1ï¼Œåªéœ€ GitHub token)")
                logger.error(f"   2. æˆ–è€…: ä» IDE æ’ä»¶ä¸­æå– Copilot session token")
                logger.error(f"   3. æˆ–è€…: ç›´æ¥ä½¿ç”¨ Anthropic API è®¿é—® Claude")
                logger.error(f"   è¯¦è§: docs/COPILOT_CLAUDE_SUPPORT.md")
            raise


def create_copilot_llm(
    model: str = "gpt-4o",
    api_key: Optional[str] = None,
    base_url: Optional[str] = None,
    temperature: float = 1.0,  # Azure AI Inference é»˜è®¤å€¼
    max_tokens: int = 8000,
    timeout: int = 120,
    **kwargs
) -> ChatCopilot:
    """
    å¿«é€Ÿåˆ›å»º GitHub Copilot LLM å®ä¾‹

    Args:
        model: æ¨¡å‹åç§°
        api_key: GitHub Token
        base_url: API åŸºç¡€ URL
        temperature: æ¸©åº¦å‚æ•°
        max_tokens: æœ€å¤§ token æ•°
        timeout: è¶…æ—¶æ—¶é—´
        **kwargs: å…¶ä»–å‚æ•°

    Returns:
        ChatCopilot å®ä¾‹

    Example:
        >>> llm = create_copilot_llm(model="gpt-4o")
        >>> response = llm.invoke("ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
    """
    return ChatCopilot(
        model=model,
        api_key=api_key,
        base_url=base_url,
        temperature=temperature,
        max_tokens=max_tokens,
        timeout=timeout,
        **kwargs
    )


# æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨
COPILOT_MODELS = [
    # ğŸ¯ æ–¹æ¡ˆ2: Copilot Chat API (æ¨¡æ‹ŸIDEæ’ä»¶ï¼Œæ”¯æŒå…¨æ¨¡å‹)
    "claude-sonnet-4.5",     # Claude Sonnet 4.5 (Anthropicæœ€æ–°) â­â­â­
    "claude-3.5-sonnet",     # Claude 3.5 Sonnet â­â­
    "gpt-5",                 # GPT-5 (æœ€æ–°æ——èˆ°æ¨¡å‹) â­â­â­
    "gpt-5-mini",            # GPT-5 Mini (è½»é‡å¿«é€Ÿ)
    "o1-preview",            # O1 Preview (æ¨ç†æ¨¡å‹) â­â­
    "o1-mini",               # O1 Mini (å¿«é€Ÿæ¨ç†)

    # ğŸ”§ æ–¹æ¡ˆ1: Azure AI Inference (å¤‡ç”¨ï¼Œå‘åå…¼å®¹)
    "gpt-4o",                # GPT-4 Optimized â­â­
    "gpt-4o-mini",           # GPT-4o Mini (å¿«é€Ÿä¸”ç»æµ) â­
    "gpt-4.1",               # GPT-4.1
    "gpt-4",                 # GPT-4
    "gpt-3.5-turbo",         # GPT-3.5 Turbo
]


def get_copilot_models():
    """è·å–æ”¯æŒçš„ Copilot æ¨¡å‹åˆ—è¡¨"""
    return COPILOT_MODELS


if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    print("ğŸ§ª æµ‹è¯• GitHub Copilot é€‚é…å™¨")

    # æ£€æŸ¥ç¯å¢ƒå˜é‡
    token = os.getenv("GITHUB_COPILOT_TOKEN")
    if not token:
        print("âŒ é”™è¯¯: æœªè®¾ç½® GITHUB_COPILOT_TOKEN ç¯å¢ƒå˜é‡")
        print("ğŸ’¡ æç¤º: è¿è¡Œ 'gh auth token' è·å– GitHub Token")
        exit(1)

    print(f"âœ… å·²æ£€æµ‹åˆ° GitHub Token (é•¿åº¦: {len(token)})")

    # åˆ›å»º LLM å®ä¾‹
    try:
        llm = create_copilot_llm(model="gpt-4o")
        print("âœ… Copilot LLM å®ä¾‹åˆ›å»ºæˆåŠŸ")

        # æµ‹è¯•è°ƒç”¨
        print("\nğŸ”„ æµ‹è¯•è°ƒç”¨...")
        response = llm.invoke("ä½ å¥½ï¼Œè¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
        print(f"âœ… è°ƒç”¨æˆåŠŸ: {response.content}")

    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

