"""
æµ‹è¯• Copilot å’Œ Copilot Business é€‚é…å™¨
"""
import os
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, project_root)

from dotenv import load_dotenv
load_dotenv()

def test_copilot_standard():
    """æµ‹è¯•æ ‡å‡† Copilot é€‚é…å™¨"""
    print("=" * 80)
    print("æµ‹è¯•æ ‡å‡† GitHub Copilot é€‚é…å™¨ (Azure AI Inference)")
    print("=" * 80)

    try:
        from tradingagents.llm_adapters.copilot_adapter import ChatCopilot

        # æ£€æŸ¥ç¯å¢ƒå˜é‡
        token = os.getenv("GITHUB_COPILOT_TOKEN")
        if not token:
            print("âš ï¸  æœªè®¾ç½® GITHUB_COPILOT_TOKENï¼Œè·³è¿‡æµ‹è¯•")
            return

        # åˆå§‹åŒ–é€‚é…å™¨
        copilot = ChatCopilot(
            model="gpt-4.1",
            temperature=1.0,
            max_tokens=500,
            timeout=120
        )

        print(f"\nâœ… åˆå§‹åŒ–æˆåŠŸï¼")
        print(f"   æ¨¡å‹: {copilot.model_name}")
        print(f"   Base URL: {copilot.openai_api_base}")

        # æµ‹è¯•ç®€å•è°ƒç”¨
        print("\næµ‹è¯•ç®€å•å¯¹è¯...")
        response = copilot.invoke("è¯·ç”¨ä¸€å¥è¯ä»‹ç»ä½ è‡ªå·±")
        print(f"\nå›ç­”: {response.content[:200]}...")

        print("\nâœ… æ ‡å‡† Copilot æµ‹è¯•é€šè¿‡ï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_copilot_business():
    """æµ‹è¯• Copilot Business é€‚é…å™¨"""
    print("\n" + "=" * 80)
    print("æµ‹è¯• GitHub Copilot Business é€‚é…å™¨")
    print("=" * 80)

    try:
        from tradingagents.llm_adapters.copilot_business_adapter import ChatCopilotBusiness

        # æ£€æŸ¥ç¯å¢ƒå˜é‡ï¼ˆæ”¯æŒä¸¤ä¸ªå˜é‡åï¼‰
        token = os.getenv("GITHUB_COPILOT_BUSINESS_TOKEN") or os.getenv("GITHUB_COPILOT_SESSION_TOKEN")

        # è°ƒè¯•ä¿¡æ¯
        print(f"ğŸ” ç¯å¢ƒå˜é‡æ£€æŸ¥:")
        print(f"   GITHUB_COPILOT_BUSINESS_TOKEN: {'å·²è®¾ç½®' if os.getenv('GITHUB_COPILOT_BUSINESS_TOKEN') else 'æœªè®¾ç½®'}")
        print(f"   GITHUB_COPILOT_SESSION_TOKEN: {'å·²è®¾ç½®' if os.getenv('GITHUB_COPILOT_SESSION_TOKEN') else 'æœªè®¾ç½®'}")

        if not token:
            print("âš ï¸  æœªè®¾ç½® Business Tokenï¼Œè·³è¿‡æµ‹è¯•")
            print("   è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½® GITHUB_COPILOT_BUSINESS_TOKEN æˆ– GITHUB_COPILOT_SESSION_TOKEN")
            return

        # å»é™¤å¯èƒ½çš„å¼•å·
        token = token.strip("'\"")
        print(f"âœ… æ£€æµ‹åˆ° Business Token (é•¿åº¦: {len(token)})")

        # æµ‹è¯•å¤šä¸ªæ¨¡å‹åç§°ï¼ˆæ ¹æ® VSCode Copilot å¯ç”¨æ¨¡å‹åˆ—è¡¨ï¼‰
        test_models = [
            # âœ… å·²éªŒè¯å¯ç”¨çš„æ¨¡å‹
            "gpt-5",                    # OpenAI GPT-5
            "gpt-4.1",                  # GPT-4.1
            "gpt-4o",                   # GPT-4 Optimized
            "claude-sonnet-4.5",        # Claude Sonnet 4.5

            # ğŸ” å¾…æµ‹è¯•çš„å…¶ä»–æ¨¡å‹ï¼ˆä» VSCode æˆªå›¾ï¼‰
            "gpt-5-mini",               # GPT-5 Mini
            "grok-code-fast-1",         # Grok Code Fast 1
            "claude-haiku-4.5",         # Claude Haiku 4.5
            "claude-sonnet-4",          # Claude Sonnet 4
            "gemini-2.5-pro",           # Gemini 2.5 Pro
            # "gemini-3-pro",             # Gemini 3 Pro (Preview)
            # "gpt-5-codex",              # GPT-5-Codex (Preview)
            "gpt-5.1",                  # GPT-5.1 (Preview)
            # "gpt-5.1-codex",            # GPT-5.1-Codex (Preview)
        ]

        print(f"\nğŸ” æµ‹è¯•å¯ç”¨æ¨¡å‹ (æ€»å…± {len(test_models)} ä¸ª)...")
        from langchain_core.messages import HumanMessage

        success_models = []
        failed_models = []

        for i, model_name in enumerate(test_models, 1):
            try:
                print(f"\n{'='*60}")
                print(f"[{i}/{len(test_models)}] æµ‹è¯•æ¨¡å‹: {model_name}")
                print(f"{'='*60}")

                # åˆå§‹åŒ–é€‚é…å™¨
                copilot_biz = ChatCopilotBusiness(
                    model=model_name,
                    temperature=1.0,
                    max_tokens=100,
                    timeout=120
                )

                # æµ‹è¯•ç®€å•è°ƒç”¨
                messages = [HumanMessage(content="ä½ å¥½ï¼Œç®€å•ä»‹ç»ä¸€ä¸‹ä½ æ˜¯ä»€ä¹ˆæ¨¡å‹")]
                response = copilot_biz._generate(messages)

                answer = response.generations[0].message.content[:150]
                print(f"âœ… {model_name} å¯ç”¨")
                print(f"   å›ç­”: {answer}...")
                success_models.append(model_name)

            except Exception as e:
                error_msg = str(e)[:200]
                print(f"âŒ {model_name} å¤±è´¥: {error_msg}")
                failed_models.append((model_name, error_msg))

        # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
        print("\n" + "="*80)
        print("ğŸ“Š æµ‹è¯•ç»Ÿè®¡")
        print("="*80)
        print(f"âœ… å¯ç”¨æ¨¡å‹ ({len(success_models)}/{len(test_models)}):")
        for model in success_models:
            print(f"   â€¢ {model}")

        if failed_models:
            print(f"\nâŒ ä¸å¯ç”¨æ¨¡å‹ ({len(failed_models)}/{len(test_models)}):")
            for model, error in failed_models:
                print(f"   â€¢ {model}: {error[:100]}")

        print("\nâœ… Copilot Business æµ‹è¯•å®Œæˆï¼")

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

def test_imports():
    """æµ‹è¯•å¯¼å…¥"""
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ¨¡å—å¯¼å…¥")
    print("=" * 80)

    try:
        from tradingagents.llm_adapters import ChatCopilot, ChatCopilotBusiness
        print("âœ… ChatCopilot å¯¼å…¥æˆåŠŸ")
        print("âœ… ChatCopilotBusiness å¯¼å…¥æˆåŠŸ")
        return True
    except Exception as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

if __name__ == "__main__":
    # æµ‹è¯•å¯¼å…¥
    if not test_imports():
        sys.exit(1)

    # æµ‹è¯•æ ‡å‡† Copilot
    test_copilot_standard()

    # æµ‹è¯• Business Copilot
    # test_copilot_business()

    print("\n" + "=" * 80)
    print("æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 80)

